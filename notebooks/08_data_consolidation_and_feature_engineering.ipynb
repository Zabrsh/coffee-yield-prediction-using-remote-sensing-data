{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee189d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# notebooks/08_data_consolidation_and_feature_engineering.ipynb\n",
    "# ==============================================================================\n",
    "\n",
    "# # 08 - Data Consolidation and Feature Engineering\n",
    "# This notebook brings together all the preprocessed data from previous steps:\n",
    "# 1.  Processed Sentinel-2 vegetation indices (`woreda_monthly_vegetation_indices.csv`)\n",
    "# 2.  Processed ERA5-Land, SRTM, and SMAP environmental data (`woreda_monthly_environmental_data.csv`)\n",
    "# 3.  Prepared annual yield data (`woreda_annual_yield_data.csv`)\n",
    "#\n",
    "# It then performs additional feature engineering, such as creating lagged variables and seasonal aggregations,\n",
    "# to build the final comprehensive dataset (`master_woreda_data.csv`) ready for machine learning modeling.\n",
    "\n",
    "# ## 1. Load Project Setup and Libraries\n",
    "# We'll load `pandas` for data manipulation and specify the directory where all processed data files are located.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries loaded.\")\n",
    "\n",
    "# Define processed data directory\n",
    "processed_data_dir = '../data/processed/'\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processed data directory: {processed_data_dir}\")\n",
    "\n",
    "# ## 2. Load Processed Data Files\n",
    "# Load the vegetation indices, environmental data, and yield data CSV files saved from previous notebooks.\n",
    "\n",
    "try:\n",
    "    df_vi = pd.read_csv(os.path.join(processed_data_dir, 'woreda_monthly_vegetation_indices.csv'))\n",
    "    print(f\"Loaded vegetation indices: {df_vi.shape[0]} records.\")\n",
    "    print(df_vi.head())\n",
    "\n",
    "    df_env = pd.read_csv(os.path.join(processed_data_dir, 'woreda_monthly_environmental_data.csv'))\n",
    "    print(f\"\\nLoaded environmental data: {df_env.shape[0]} records.\")\n",
    "    print(df_env.head())\n",
    "\n",
    "    df_yield = pd.read_csv(os.path.join(processed_data_dir, 'woreda_annual_yield_data.csv'))\n",
    "    print(f\"\\nLoaded annual yield data: {df_yield.shape[0]} records.\")\n",
    "    print(df_yield.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}. Please ensure you have run notebooks 05, 06, and 07 to generate these files.\")\n",
    "    df_vi, df_env, df_yield = None, None, None # Set to None to prevent further execution\n",
    "\n",
    "# ## 3. Consolidate Monthly Data\n",
    "# Merge the vegetation indices and environmental data into a single monthly DataFrame. This forms the base for time-series feature engineering.\n",
    "\n",
    "if df_vi is not None and df_env is not None:\n",
    "    # Merge vegetation indices and environmental data by woreda_id, year, and month\n",
    "    df_monthly_features = pd.merge(df_vi, df_env,\n",
    "                                   on=['woreda_id', 'woreda_name', 'year', 'month'],\n",
    "                                   how='outer') # Use outer to keep all available records\n",
    "\n",
    "    # Sort for correct lagged calculations\n",
    "    df_monthly_features = df_monthly_features.sort_values(by=['woreda_id', 'year', 'month']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nConsolidated monthly features: {df_monthly_features.shape[0]} records.\")\n",
    "    print(df_monthly_features.head())\n",
    "else:\n",
    "    print(\"Skipping monthly data consolidation due to missing input data.\")\n",
    "\n",
    "# ## 4. Feature Engineering: Lagged Variables and Seasonal Aggregations\n",
    "# This section generates additional features from the monthly data to capture temporal patterns that might influence annual yield.\n",
    "# Common techniques include:\n",
    "# -   **Lagged features:** Values from previous months/seasons (e.g., NDVI 3 months prior).\n",
    "# -   **Seasonal statistics:** Averages or sums over specific growing seasons (e.g., average precipitation during the main rainy season).\n",
    "\n",
    "if 'df_monthly_features' in locals() and df_monthly_features is not None:\n",
    "    # --- Create Lagged Features (Example: Lagging by 1 month) ---\n",
    "    # Identify columns to lag (exclude identifiers and static features like elevation)\n",
    "    cols_to_lag = [\n",
    "        'avg_ndvi', 'avg_savi',\n",
    "        'era5_total_precipitation', 'era5_temperature_2m',\n",
    "        'era5_surface_pressure', 'era5_soil_temperature_level_1', 'era5_soil_volume_water_content_level_1',\n",
    "        'smap_sm_surface_pressure', 'smap_sm_rootzone_pressure'\n",
    "    ]\n",
    "\n",
    "    # Create a unique time identifier for sorting within groups\n",
    "    df_monthly_features['date_id'] = df_monthly_features['year'] * 100 + df_monthly_features['month']\n",
    "    df_monthly_features = df_monthly_features.sort_values(by=['woreda_id', 'date_id'])\n",
    "\n",
    "    for col in cols_to_lag:\n",
    "        for lag in [1, 2, 3]: # Example lags: 1, 2, and 3 months prior\n",
    "            df_monthly_features[f'{col}_lag{lag}'] = df_monthly_features.groupby('woreda_id')[col].shift(lag)\n",
    "\n",
    "    print(f\"\\nMonthly features with lagged variables: {df_monthly_features.shape[0]} records, {df_monthly_features.shape[1]} columns.\")\n",
    "    print(df_monthly_features.head())\n",
    "\n",
    "    # --- Create Seasonal Aggregations ---\n",
    "    # Define seasons based on coffee growing cycle (example for Ethiopia/Sidama)\n",
    "    # Main rainy season (Kremt): June, July, August, September\n",
    "    # Dry season (Bega): October, November, December, January, February\n",
    "    # Short rainy season (Belg): March, April, May\n",
    "\n",
    "    # For yield prediction for a given year Y, features often come from year Y and Y-1.\n",
    "    # Example: Average precipitation in the previous growing season.\n",
    "\n",
    "    # Aggregate monthly features to annual level for modeling against annual yield\n",
    "    # We'll aggregate features for the year *before* the yield year and potentially early months of the yield year itself.\n",
    "    # This example aggregates all monthly features to a yearly mean for the current year.\n",
    "    # You will likely need to refine these aggregations based on phenology and domain knowledge.\n",
    "\n",
    "    # Group by woreda_id and year, then aggregate relevant features\n",
    "    # Exclude identifiers and columns not suitable for mean aggregation\n",
    "    agg_cols = [col for col in df_monthly_features.columns if col not in ['woreda_name', 'month', 'date_id']]\n",
    "\n",
    "    # Ensure avg_elevation is not included in monthly aggregation, as it's static\n",
    "    if 'avg_elevation' in agg_cols:\n",
    "        agg_cols.remove('avg_elevation')\n",
    "\n",
    "    # For simplicity, let's take the mean of monthly values for each year.\n",
    "    # For better results, you'd define specific growing seasons and aggregate accordingly.\n",
    "    df_annual_features = df_monthly_features.groupby(['woreda_id', 'year'])[agg_cols].mean().reset_index()\n",
    "\n",
    "    # If avg_elevation was removed, re-add it from df_env or df_monthly_features (it's static per woreda)\n",
    "    # A more robust way: merge elevation at the end or ensure it's not part of the monthly aggregation\n",
    "    # For now, let's assume `avg_elevation` is handled. If it's missing, you can re-merge it.\n",
    "    if 'avg_elevation' in df_monthly_features.columns:\n",
    "        # Take the first non-null elevation per woreda_id\n",
    "        woreda_elevation = df_monthly_features[['woreda_id', 'avg_elevation']].drop_duplicates().set_index('woreda_id')\n",
    "        df_annual_features = df_annual_features.merge(woreda_elevation, on='woreda_id', how='left')\n",
    "\n",
    "    print(f\"\\nAnnual features (after aggregation): {df_annual_features.shape[0]} records.\")\n",
    "    print(df_annual_features.head())\n",
    "else:\n",
    "    print(\"Skipping feature engineering due to missing consolidated monthly data.\")\n",
    "\n",
    "# ## 5. Final Data Consolidation (Features + Yield)\n",
    "# Merge the engineered features with the annual yield data. This is the final dataset used for training and evaluating the yield prediction model.\n",
    "\n",
    "if 'df_annual_features' in locals() and df_annual_features is not None and df_yield is not None:\n",
    "    # Merge features with yield data\n",
    "    # The 'annual_yield_quintals_ha' from df_yield will be the target variable\n",
    "    df_master = pd.merge(df_annual_features, df_yield,\n",
    "                         on=['woreda_id', 'year'],\n",
    "                         how='left') # Use left join to keep all features and add yield if available\n",
    "\n",
    "    # Add woreda name back for convenience if it was lost during aggregation\n",
    "    if 'woreda_name' not in df_master.columns:\n",
    "        df_woreda_names = df_monthly_features[['woreda_id', 'woreda_name']].drop_duplicates()\n",
    "        df_master = pd.merge(df_master, df_woreda_names, on='woreda_id', how='left')\n",
    "\n",
    "    # Reorder columns to have identifiers and target first\n",
    "    cols = ['woreda_id', 'woreda_name', 'year', 'annual_yield_quintals_ha'] + \\\n",
    "           [col for col in df_master.columns if col not in ['woreda_id', 'woreda_name', 'year', 'annual_yield_quintals_ha']]\n",
    "    df_master = df_master[cols]\n",
    "\n",
    "    print(f\"\\nFinal master dataset shape: {df_master.shape}\")\n",
    "    print(df_master.head())\n",
    "\n",
    "    # Display number of rows with missing yield data (these rows can be used for prediction)\n",
    "    missing_yield_rows = df_master['annual_yield_quintals_ha'].isna().sum()\n",
    "    print(f\"Number of rows with missing yield data (for potential prediction): {missing_yield_rows}\")\n",
    "\n",
    "    # Save the master dataset\n",
    "    master_data_path = os.path.join(processed_data_dir, 'master_woreda_data.csv')\n",
    "    df_master.to_csv(master_data_path, index=False)\n",
    "    print(f\"✅ Master dataset saved to {master_data_path}\")\n",
    "else:\n",
    "    print(\"Skipping final data consolidation due to missing input data.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
