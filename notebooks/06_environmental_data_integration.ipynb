{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# notebooks/06_environmental_data_integration.ipynb\n",
    "# ==============================================================================\n",
    "\n",
    "# # 06 - Environmental Data Integration\n",
    "# This notebook integrates the exported ERA5-Land, SRTM, and SMAP data.\n",
    "# It involves:\n",
    "# 1.  Downloading the exported CSV files from GCS for ERA5-Land, SRTM, and SMAP.\n",
    "# 2.  Consolidating the ERA5-Land and SMAP data into monthly time-series per woreda.\n",
    "# 3.  Integrating the static SRTM (elevation) data.\n",
    "# 4.  Saving the combined environmental features.\n",
    "\n",
    "# ## 1. Load Project Setup and Libraries\n",
    "# Import `pandas`, `numpy`, `os`, and custom GCS I/O module.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path to import custom modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.gcs_io import download_gcs_files\n",
    "\n",
    "print(\"Libraries and custom modules loaded.\")\n",
    "\n",
    "# Define common variables and paths\n",
    "BUCKET_NAME = 'bensa-coffee-yield' # Ensure this matches your GCS bucket\n",
    "processed_data_dir = '../data/processed/'\n",
    "gee_exports_dir = '../data/gee_exports/'\n",
    "\n",
    "# GCS folders for exports\n",
    "era5_export_gcs_folder = 'gee_exports/era5/'\n",
    "srtm_export_gcs_folder = 'gee_exports/srtm/'\n",
    "smap_export_gcs_folder = 'gee_exports/smap/'\n",
    "\n",
    "# Local download directories\n",
    "era5_download_dir = os.path.join(gee_exports_dir, 'era5/')\n",
    "srtm_download_dir = os.path.join(gee_exports_dir, 'srtm/')\n",
    "smap_download_dir = os.path.join(gee_exports_dir, 'smap/')\n",
    "\n",
    "os.makedirs(era5_download_dir, exist_ok=True)\n",
    "os.makedirs(srtm_download_dir, exist_ok=True)\n",
    "os.makedirs(smap_download_dir, exist_ok=True)\n",
    "\n",
    "# Load woreda boundaries for woreda_name information\n",
    "PROCESSED_WOREDAS_GEOJSON_PATH = os.path.join(processed_data_dir, 'sidama_woredas.geojson')\n",
    "gdf_woredas = None\n",
    "try:\n",
    "    gdf_woredas = gpd.read_file(PROCESSED_WOREDAS_GEOJSON_PATH)\n",
    "    if 'Woreda_ID' not in gdf_woredas.columns or 'Woreda Name' not in gdf_woredas.columns:\n",
    "        raise ValueError(\"Woredas GeoDataFrame must contain 'Woreda_ID' and 'Woreda Name' columns.\")\n",
    "    gdf_woredas['Woreda_ID'] = gdf_woredas['Woreda_ID'].astype(str) # Ensure ID is string\n",
    "    print(f\"Loaded GeoDataFrame with {len(gdf_woredas)} woredas.\")\n",
    "    woreda_name_map = gdf_woredas.set_index('Woreda_ID')['Woreda Name'].to_dict()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{PROCESSED_WOREDAS_GEOJSON_PATH}' not found. Please run '00_setup_and_common_data_loading.ipynb' first.\")\n",
    "    gdf_woredas = None\n",
    "except ValueError as e:\n",
    "    print(f\"Data error in woreda GeoDataFrame: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred loading woreda data: {e}\")\n",
    "\n",
    "\n",
    "# ## 2. Download Exported CSVs from GCS\n",
    "# Download ERA5, SRTM, and SMAP CSV files.\n",
    "\n",
    "if gdf_woredas is not None:\n",
    "    print(f\"\\nDownloading ERA5 export files from GCS folder '{era5_export_gcs_folder}' to '{era5_download_dir}'...\")\n",
    "    download_gcs_files(BUCKET_NAME, era5_export_gcs_folder, era5_download_dir)\n",
    "    print(\"✅ ERA5 CSV downloads complete.\")\n",
    "\n",
    "    print(f\"\\nDownloading SRTM export files from GCS folder '{srtm_export_gcs_folder}' to '{srtm_download_dir}'...\")\n",
    "    download_gcs_files(BUCKET_NAME, srtm_export_gcs_folder, srtm_download_dir)\n",
    "    print(\"✅ SRTM CSV downloads complete.\")\n",
    "\n",
    "    print(f\"\\nDownloading SMAP export files from GCS folder '{smap_export_gcs_folder}' to '{smap_download_dir}'...\")\n",
    "    download_gcs_files(BUCKET_NAME, smap_export_gcs_folder, smap_download_dir)\n",
    "    print(\"✅ SMAP CSV downloads complete.\")\n",
    "else:\n",
    "    print(\"Skipping environmental CSV downloads due to missing woreda data.\")\n",
    "\n",
    "# ## 3. Process ERA5-Land Data\n",
    "# Consolidate and clean ERA5-Land data.\n",
    "\n",
    "if gdf_woredas is not None:\n",
    "    all_era5_data = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    print(\"\\nProcessing downloaded ERA5-Land CSVs...\")\n",
    "    for filename in os.listdir(era5_download_dir):\n",
    "        if filename.startswith('era5_') and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(era5_download_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Handle woreda_id extraction from filename if not in column directly\n",
    "                if 'Woreda_ID' in df.columns:\n",
    "                    df = df.rename(columns={'Woreda_ID': 'woreda_id'})\n",
    "                elif 'woreda_id' not in df.columns:\n",
    "                    woreda_id_from_filename = filename.replace('era5_', '').replace('.csv', '')\n",
    "                    df['woreda_id'] = woreda_id_from_filename\n",
    "\n",
    "                if 'woreda_id' not in df.columns:\n",
    "                    raise KeyError(f\"'woreda_id' column not found or inferrable in {filename}\")\n",
    "\n",
    "                df['woreda_id'] = df['woreda_id'].astype(str)\n",
    "                df['year'] = df['year'].astype(int)\n",
    "                df['month'] = df['month'].astype(int)\n",
    "\n",
    "                # Select relevant ERA5 columns. Adjust names if needed.\n",
    "                era5_cols = ['woreda_id', 'year', 'month',\n",
    "                             'era5_total_precipitation', 'era5_temperature_2m',\n",
    "                             'era5_surface_pressure', 'era5_soil_temperature_level_1',\n",
    "                             'era5_soil_volume_water_content_level_1']\n",
    "                \n",
    "                df_subset = df[era5_cols].copy()\n",
    "                all_era5_data.append(df_subset)\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {filename}: {e}\")\n",
    "                error_count += 1\n",
    "\n",
    "    if all_era5_data:\n",
    "        df_era5 = pd.concat(all_era5_data, ignore_index=True)\n",
    "        df_era5['woreda_name'] = df_era5['woreda_id'].map(woreda_name_map)\n",
    "        df_era5.dropna(subset=['woreda_name'], inplace=True) # Drop if woreda_name missing\n",
    "        df_era5 = df_era5.groupby(['woreda_id', 'woreda_name', 'year', 'month']).mean().reset_index()\n",
    "        df_era5 = df_era5.sort_values(by=['woreda_id', 'year', 'month']).reset_index(drop=True)\n",
    "        print(f\"\\n✅ Consolidated monthly ERA5 data: {df_era5.shape[0]} records.\")\n",
    "        print(df_era5.head())\n",
    "    else:\n",
    "        print(\"No ERA5 CSVs were processed successfully.\")\n",
    "        df_era5 = None\n",
    "else:\n",
    "    print(\"Skipping processing ERA5 data due to missing woreda data.\")\n",
    "\n",
    "# ## 4. Process SMAP Data\n",
    "# Consolidate and clean SMAP data.\n",
    "\n",
    "if gdf_woredas is not None:\n",
    "    all_smap_data = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    print(\"\\nProcessing downloaded SMAP CSVs...\")\n",
    "    for filename in os.listdir(smap_download_dir):\n",
    "        if filename.startswith('smap_') and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(smap_download_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Handle woreda_id extraction from filename if not in column directly\n",
    "                if 'Woreda_ID' in df.columns:\n",
    "                    df = df.rename(columns={'Woreda_ID': 'woreda_id'})\n",
    "                elif 'woreda_id' not in df.columns:\n",
    "                    woreda_id_from_filename = filename.replace('smap_', '').replace('.csv', '')\n",
    "                    df['woreda_id'] = woreda_id_from_filename\n",
    "\n",
    "                if 'woreda_id' not in df.columns:\n",
    "                    raise KeyError(f\"'woreda_id' column not found or inferrable in {filename}\")\n",
    "\n",
    "                df['woreda_id'] = df['woreda_id'].astype(str)\n",
    "                df['year'] = df['year'].astype(int)\n",
    "                df['month'] = df['month'].astype(int)\n",
    "\n",
    "                # Select relevant SMAP columns. Adjust names if needed.\n",
    "                smap_cols = ['woreda_id', 'year', 'month', 'sm_surface', 'sm_rootzone']\n",
    "                df_subset = df[smap_cols].copy()\n",
    "                \n",
    "                # Rename SMAP bands for clarity and consistency\n",
    "                df_subset = df_subset.rename(columns={'sm_surface': 'smap_sm_surface_pressure', # Original notes might have used 'pressure' incorrectly here\n",
    "                                                      'sm_rootzone': 'smap_sm_rootzone_pressure'}) # These are volumetric soil moisture, not pressure\n",
    "\n",
    "                all_smap_data.append(df_subset)\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {filename}: {e}\")\n",
    "                error_count += 1\n",
    "\n",
    "    if all_smap_data:\n",
    "        df_smap = pd.concat(all_smap_data, ignore_index=True)\n",
    "        df_smap['woreda_name'] = df_smap['woreda_id'].map(woreda_name_map)\n",
    "        df_smap.dropna(subset=['woreda_name'], inplace=True)\n",
    "        df_smap = df_smap.groupby(['woreda_id', 'woreda_name', 'year', 'month']).mean().reset_index()\n",
    "        df_smap = df_smap.sort_values(by=['woreda_id', 'year', 'month']).reset_index(drop=True)\n",
    "        print(f\"\\n✅ Consolidated monthly SMAP data: {df_smap.shape[0]} records.\")\n",
    "        print(df_smap.head())\n",
    "    else:\n",
    "        print(\"No SMAP CSVs were processed successfully.\")\n",
    "        df_smap = None\n",
    "else:\n",
    "    print(\"Skipping processing SMAP data due to missing woreda data.\")\n",
    "\n",
    "# ## 5. Process SRTM Elevation Data\n",
    "# Consolidate SRTM elevation data (which is static per woreda).\n",
    "\n",
    "if gdf_woredas is not None:\n",
    "    all_srtm_data = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    print(\"\\nProcessing downloaded SRTM CSVs...\")\n",
    "    for filename in os.listdir(srtm_download_dir):\n",
    "        if filename.startswith('srtm_elevation_') and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(srtm_download_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Handle woreda_id extraction from filename if not in column directly\n",
    "                if 'Woreda_ID' in df.columns:\n",
    "                    df = df.rename(columns={'Woreda_ID': 'woreda_id'})\n",
    "                elif 'woreda_id' not in df.columns:\n",
    "                    woreda_id_from_filename = filename.replace('srtm_elevation_', '').replace('.csv', '')\n",
    "                    df['woreda_id'] = woreda_id_from_filename\n",
    "\n",
    "                if 'woreda_id' not in df.columns:\n",
    "                    raise KeyError(f\"'woreda_id' column not found or inferrable in {filename}\")\n",
    "\n",
    "                df['woreda_id'] = df['woreda_id'].astype(str)\n",
    "                \n",
    "                # Select elevation column and rename\n",
    "                df_subset = df[['woreda_id', 'elevation']].copy()\n",
    "                df_subset = df_subset.rename(columns={'elevation': 'avg_elevation'})\n",
    "\n",
    "                all_srtm_data.append(df_subset)\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {filename}: {e}\")\n",
    "                error_count += 1\n",
    "\n",
    "    if all_srtm_data:\n",
    "        df_srtm = pd.concat(all_srtm_data, ignore_index=True)\n",
    "        df_srtm['woreda_name'] = df_srtm['woreda_id'].map(woreda_name_map)\n",
    "        df_srtm.dropna(subset=['woreda_name'], inplace=True)\n",
    "        df_srtm = df_srtm.drop_duplicates(subset=['woreda_id']).reset_index(drop=True) # Elevation is static per woreda\n",
    "        print(f\"\\n✅ Consolidated SRTM elevation data: {df_srtm.shape[0]} records.\")\n",
    "        print(df_srtm.head())\n",
    "    else:\n",
    "        print(\"No SRTM CSVs were processed successfully.\")\n",
    "        df_srtm = None\n",
    "else:\n",
    "    print(\"Skipping processing SRTM data due to missing woreda data.\")\n",
    "\n",
    "# ## 6. Merge All Environmental Data\n",
    "# Combine ERA5, SMAP (monthly) and SRTM (static) into a single DataFrame.\n",
    "\n",
    "if df_era5 is not None and df_smap is not None and df_srtm is not None:\n",
    "    # Merge monthly ERA5 and SMAP data first\n",
    "    df_monthly_env = pd.merge(df_era5, df_smap, on=['woreda_id', 'woreda_name', 'year', 'month'], how='outer')\n",
    "    \n",
    "    # Merge with static elevation data (df_srtm)\n",
    "    # Elevation is static, so it just needs to be joined by woreda_id\n",
    "    df_monthly_env = pd.merge(df_monthly_env, df_srtm[['woreda_id', 'avg_elevation']], on='woreda_id', how='left')\n",
    "\n",
    "    df_monthly_env = df_monthly_env.sort_values(by=['woreda_id', 'year', 'month']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n✅ Consolidated all monthly environmental data: {df_monthly_env.shape[0]} records.\")\n",
    "    print(df_monthly_env.head())\n",
    "\n",
    "    # Save the consolidated data\n",
    "    output_path = os.path.join(processed_data_dir, 'woreda_monthly_environmental_data.csv')\n",
    "    df_monthly_env.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Consolidated monthly environmental data saved to {output_path}\")\n",
    "else:\n",
    "    print(\"Skipping full environmental data consolidation due to missing sub-datasets.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
