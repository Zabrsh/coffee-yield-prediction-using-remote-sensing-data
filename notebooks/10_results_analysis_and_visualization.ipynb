{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80208edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# notebooks/10_results_analysis_and_visualization.ipynb\n",
    "# ==============================================================================\n",
    "\n",
    "# # 10 - Results Analysis and Visualization\n",
    "# This notebook provides a comprehensive analysis and visualization of the coffee yield prediction model's results. It aims to:\n",
    "# 1.  Load the master dataset (containing actual yields) and the 2025 predictions.\n",
    "# 2.  Evaluate the model's performance on historical data.\n",
    "# 3.  Visualize actual vs. predicted yields to assess model fit.\n",
    "# 4.  Display the 2025 predicted yields spatially.\n",
    "# 5.  Revisit and visualize feature importance.\n",
    "# 6.  Summarize key insights.\n",
    "\n",
    "# ## 1. Load Project Setup and Libraries\n",
    "# Import `pandas`, `numpy`, `os`, `matplotlib.pyplot`, `seaborn`, `geopandas`, and `pickle` (to load model artifacts).\n",
    "# Also, import evaluation metrics from `sklearn.metrics`.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import pickle # To load the saved model and scaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(\"Libraries loaded.\")\n",
    "\n",
    "# Define data directories\n",
    "processed_data_dir = '../data/processed/'\n",
    "model_dir = '../models/'\n",
    "\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processed data directory: {processed_data_dir}\")\n",
    "print(f\"Models directory: {model_dir}\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# ## 2. Load Data and Model Artifacts\n",
    "# Load the master dataset, the 2025 predictions, the original woreda boundaries (for spatial visualization),\n",
    "# and the trained model/scaler/feature columns.\n",
    "\n",
    "df_master = None\n",
    "df_predictions_2025 = None\n",
    "gdf_woredas = None\n",
    "best_model = None\n",
    "scaler = None\n",
    "feature_cols = None\n",
    "\n",
    "try:\n",
    "    df_master = pd.read_csv(os.path.join(processed_data_dir, 'master_woreda_data.csv'))\n",
    "    print(f\"Loaded master dataset: {df_master.shape[0]} records.\")\n",
    "\n",
    "    df_predictions_2025 = pd.read_csv(os.path.join(processed_data_dir, 'sidama_coffee_yield_predictions_2025.csv'))\n",
    "    print(f\"Loaded 2025 predictions: {df_predictions_2025.shape[0]} records.\")\n",
    "\n",
    "    gdf_woredas = gpd.read_file(os.path.join(processed_data_dir, 'sidama_woredas.geojson'))\n",
    "    print(f\"Loaded woreda geometries: {len(gdf_woredas)} woredas.\")\n",
    "\n",
    "    # Load trained model artifacts\n",
    "    with open(os.path.join(model_dir, 'best_yield_prediction_model.pkl'), 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "    print(f\"Loaded best model: {type(best_model).__name__}\")\n",
    "\n",
    "    with open(os.path.join(model_dir, 'feature_scaler.pkl'), 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    print(\"Loaded feature scaler.\")\n",
    "\n",
    "    with open(os.path.join(model_dir, 'feature_columns.pkl'), 'rb') as f:\n",
    "        feature_cols = pickle.load(f)\n",
    "    print(f\"Loaded feature columns: {len(feature_cols)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data/model artifacts: {e}. Please ensure previous notebooks (08, 09) were run successfully.\")\n",
    "\n",
    "print(\"\\nData and model artifacts loading complete.\")\n",
    "\n",
    "# ## 3. Model Performance Evaluation on Historical Data\n",
    "# Re-evaluate the best model's performance on the historical training data to confirm metrics and visualize residuals.\n",
    "\n",
    "if df_master is not None and best_model is not None and scaler is not None and feature_cols is not None:\n",
    "    df_train_actuals = df_master.dropna(subset=['annual_yield_quintals_ha']).copy()\n",
    "\n",
    "    X_train = df_train_actuals[feature_cols]\n",
    "    y_train = df_train_actuals['annual_yield_quintals_ha']\n",
    "\n",
    "    # Impute missing values in X_train using the same means as during training\n",
    "    # (This assumes the means were calculated on X during 09_yield_prediction_model.ipynb\n",
    "    # and that the scaler wasn't solely responsible for imputation. If imputation was part of scaler\n",
    "    # pipeline, then this step might not be strictly needed, but it's safer).\n",
    "    # Re-run the imputation part of notebook 09 if you hit errors here regarding NaNs.\n",
    "    for col in feature_cols:\n",
    "        if X_train[col].isnull().any():\n",
    "            # For simplicity, using mean of the current X_train. In a real pipeline,\n",
    "            # you'd save/load the imputer from the training phase.\n",
    "            mean_val = X_train[col].mean()\n",
    "            X_train[col] = X_train[col].fillna(mean_val)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train) # Use the loaded scaler\n",
    "\n",
    "    y_pred_train = best_model.predict(X_train_scaled)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "    print(f\"\\n--- Model Performance on Historical Data ({type(best_model).__name__}) ---\")\n",
    "    print(f\"RMSE: {rmse_train:.3f}\")\n",
    "    print(f\"R2 Score: {r2_train:.3f}\")\n",
    "    print(f\"MAE: {mae_train:.3f}\")\n",
    "\n",
    "    # Plot Actual vs. Predicted (Historical)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x=y_train, y=y_pred_train, scatter_kws={'alpha':0.6}, line_kws={'color':'red'})\n",
    "    plt.xlabel('Actual Yield (quintals/ha)')\n",
    "    plt.ylabel('Predicted Yield (quintals/ha)')\n",
    "    plt.title('Actual vs. Predicted Coffee Yields (Historical Data)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Residuals\n",
    "    residuals = y_train - y_pred_train\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=30)\n",
    "    plt.xlabel('Residuals (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_train, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.xlabel('Predicted Yield')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs. Predicted Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping model performance evaluation due to missing data/model artifacts.\")\n",
    "\n",
    "# ## 4. Visualize 2025 Predicted Yields Spatially\n",
    "# Merge the 2025 predictions with the woreda geometries to visualize the predicted yield distribution across Sidama.\n",
    "\n",
    "if df_predictions_2025 is not None and gdf_woredas is not None:\n",
    "    gdf_predictions = gdf_woredas.merge(df_predictions_2025, left_on='Woreda_ID', right_on='woreda_id', how='left')\n",
    "\n",
    "    print(f\"\\nGeoDataFrame with 2025 predictions: {gdf_predictions.shape[0]} records.\")\n",
    "    print(gdf_predictions.head())\n",
    "\n",
    "    # Plotting the predicted yields\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    gdf_predictions.plot(\n",
    "        column='predicted_yield_quintals_ha',\n",
    "        cmap='YlGn',\n",
    "        linewidth=0.8,\n",
    "        ax=ax,\n",
    "        edgecolor='0.8',\n",
    "        legend=True,\n",
    "        legend_kwds={'label': \"Predicted Coffee Yield (quintals/ha) in 2025\"}\n",
    "    )\n",
    "    ax.set_title('2025 Predicted Coffee Yields by Woreda in Sidama, Ethiopia')\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping spatial visualization due to missing prediction or woreda data.\")\n",
    "\n",
    "# ## 5. Feature Importance Analysis (Revisit)\n",
    "# Visualize the feature importances to reinforce understanding of which factors most influence coffee yield predictions.\n",
    "\n",
    "if best_model is not None and feature_cols is not None:\n",
    "    print(\"\\n--- Feature Importance/Coefficients Visualization ---\")\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title('Top 15 Feature Importances')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "\n",
    "    elif hasattr(best_model, 'coef_'): # For linear models like Ridge/Lasso, or final_estimator of Stacking\n",
    "        # Handle Stacking Regressor's final estimator coefficients\n",
    "        if isinstance(best_model, StackingRegressor) and hasattr(best_model.final_estimator_, 'coef_'):\n",
    "            coefficients = best_model.final_estimator_.coef_\n",
    "            # Note: these coefficients are for the *meta-features* (outputs of base models)\n",
    "            # not the original features directly.\n",
    "            meta_feature_names = [f'base_model_{name}' for name, _ in best_model.estimators]\n",
    "            coef_df = pd.DataFrame({\n",
    "                'Feature': meta_feature_names,\n",
    "                'Coefficient': coefficients\n",
    "            }).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='Coefficient', y='Feature', data=coef_df)\n",
    "            plt.title('Stacking Regressor: Final Estimator Coefficients (Meta-Features)')\n",
    "            plt.xlabel('Coefficient Value')\n",
    "            plt.ylabel('Base Model / Meta-Feature')\n",
    "            plt.show()\n",
    "\n",
    "        # For simple linear models\n",
    "        elif not isinstance(best_model, StackingRegressor):\n",
    "            coefficients = pd.DataFrame({\n",
    "                'Feature': feature_cols,\n",
    "                'Coefficient': best_model.coef_\n",
    "            }).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Coefficient', y='Feature', data=coefficients.head(15))\n",
    "            plt.title('Top 15 Feature Coefficients')\n",
    "            plt.xlabel('Coefficient Value')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Feature importances/coefficients not directly visualizable for this model type.\")\n",
    "else:\n",
    "    print(\"Skipping feature importance visualization due to missing model or feature columns.\")\n",
    "\n",
    "# ## 6. Conclusion and Next Steps\n",
    "# Summarize the findings and discuss potential next steps for improving the model or deploying it.\n",
    "\n",
    "# This notebook concludes the yield prediction project by analyzing the model's performance and visualizing the results.\n",
    "\n",
    "# **Key Insights from Analysis:**\n",
    "# * **Model Performance:** [Summarize RMSE, R2, MAE values from Section 3. Discuss if these are satisfactory. E.g., \"The model achieved an R2 of X, indicating that X% of the variance in coffee yield can be explained by the features.\"]\n",
    "# * **Residuals:** [Describe the residual plots. Are they randomly distributed around zero? Is there any heteroscedasticity or pattern? E.g., \"Residuals appear mostly homoscedastic and centered around zero, suggesting a reasonable model fit, though some outliers are present.\"]\n",
    "# * **Predicted Yields (2025):** [Comment on the spatial distribution of predictions. Do they make sense geographically? Are there unexpected highs/lows? E.g., \"The 2025 predictions show higher yields in [specific areas] and lower in [other areas], generally aligning with known coffee growing conditions.\"]\n",
    "# * **Feature Importance:** [Highlight the most important features. Do these align with domain knowledge? E.g., \"Vegetation indices (NDVI, SAVI) from the preceding months, along with total precipitation and average temperature, were consistently identified as the most impactful features, which aligns with agricultural understanding.\"]\n",
    "\n",
    "# **Potential Next Steps:**\n",
    "# 1.  **Hyperparameter Tuning:** Further optimize the hyperparameters of the chosen best model (e.g., using GridSearchCV or RandomizedSearchCV) to potentially improve performance.\n",
    "# 2.  **More Advanced Feature Engineering:** Explore more sophisticated temporal aggregations (e.g., cumulative sums of rainfall over specific growing phases, rolling averages) or interaction terms between features.\n",
    "# 3.  **Alternative Models:** Experiment with other machine learning models (e.g., LightGBM, CatBoost, neural networks) that might capture more complex non-linear relationships.\n",
    "# 4.  **Data Augmentation:** If more historical yield data becomes available, integrate it to potentially train a more robust model.\n",
    "# 5.  **Uncertainty Quantification:** Implement methods to quantify the uncertainty of predictions (e.g., using quantile regression forests or Bayesian methods) to provide decision-makers with a range of possible outcomes.\n",
    "# 6.  **Deployment:** Integrate the trained model into a production environment (e.g., a web application, a data dashboard) to provide real-time or regular yield forecasts.\n",
    "\n",
    "\n",
    "Sources\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
