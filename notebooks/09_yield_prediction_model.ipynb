{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# notebooks/09_yield_prediction_model.ipynb\n",
    "# ==============================================================================\n",
    "\n",
    "# # 09 - Yield Prediction Model: Training, Evaluation, and Prediction\n",
    "# This notebook is the culmination of the data pipeline. It focuses on:\n",
    "# 1.  Loading the consolidated master dataset (`master_woreda_data.csv`).\n",
    "# 2.  Preparing the data for machine learning (handling missing values, feature scaling).\n",
    "# 3.  Training and evaluating multiple regression models (Random Forest, XGBoost, Ridge, Lasso, Stacking).\n",
    "# 4.  Selecting the best performing model based on cross-validation metrics.\n",
    "# 5.  Using the best model to make yield predictions for 2025.\n",
    "# 6.  Analyzing feature importance to understand which variables drive yield predictions.\n",
    "\n",
    "# ## 1. Load Project Setup and Libraries\n",
    "# We'll load `pandas`, `numpy`, and various modules from `sklearn` and `xgboost` for model training and evaluation. `pickle` is used for saving the trained model.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"Libraries loaded.\")\n",
    "\n",
    "# Define processed data directory\n",
    "processed_data_dir = '../data/processed/'\n",
    "model_dir = '../models/'\n",
    "\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processed data directory: {processed_data_dir}\")\n",
    "print(f\"Models will be saved to: {model_dir}\")\n",
    "\n",
    "# ## 2. Load Master Dataset\n",
    "# Load the `master_woreda_data.csv` file, which contains all the integrated features and the target yield variable.\n",
    "\n",
    "master_data_path = os.path.join(processed_data_dir, 'master_woreda_data.csv')\n",
    "\n",
    "try:\n",
    "    df_master = pd.read_csv(master_data_path)\n",
    "    print(f\"Loaded master dataset: {df_master.shape[0]} records, {df_master.shape[1]} columns.\")\n",
    "    print(df_master.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading master data: {e}. Please ensure you have run notebook 08 to generate this file.\")\n",
    "    df_master = None\n",
    "\n",
    "# ## 3. Data Preparation for Modeling\n",
    "# This section involves:\n",
    "# -   Separating features (X) and target (y).\n",
    "# -   Handling missing values (e.g., imputation).\n",
    "# -   Scaling numerical features.\n",
    "\n",
    "if df_master is not None:\n",
    "    # Separate data for training (where yield is available) and prediction (2025 data)\n",
    "    df_train = df_master.dropna(subset=['annual_yield_quintals_ha']).copy()\n",
    "    df_2025 = df_master[df_master['year'] == 2025].copy()\n",
    "\n",
    "    print(f\"\\nTraining data (with yield): {df_train.shape[0]} records.\")\n",
    "    print(f\"2025 data (for prediction): {df_2025.shape[0]} records.\")\n",
    "\n",
    "    # Define features and target\n",
    "    # Exclude identifiers and the target itself from features\n",
    "    feature_cols = [col for col in df_train.columns if col not in ['woreda_id', 'woreda_name', 'year', 'annual_yield_quintals_ha']]\n",
    "    target_col = 'annual_yield_quintals_ha'\n",
    "\n",
    "    X = df_train[feature_cols]\n",
    "    y = df_train[target_col]\n",
    "\n",
    "    X_2025 = df_2025[feature_cols]\n",
    "\n",
    "    # --- Handle Missing Values (Imputation) ---\n",
    "    # For simplicity, using mean imputation. Consider more sophisticated methods like MICE or KNN imputation.\n",
    "    # It's crucial to fit the imputer ONLY on training data and transform both train and test/prediction data.\n",
    "    for col in feature_cols:\n",
    "        if X[col].isnull().any():\n",
    "            mean_val = X[col].mean()\n",
    "            X[col] = X[col].fillna(mean_val)\n",
    "            X_2025[col] = X_2025[col].fillna(mean_val) # Apply same imputation to 2025 data\n",
    "            print(f\"  Imputed missing values in '{col}' with mean {mean_val:.2f}\")\n",
    "\n",
    "    # Check for any remaining NaNs (should be none in X and X_2025 features now)\n",
    "    if X.isnull().sum().sum() > 0 or X_2025.isnull().sum().sum() > 0:\n",
    "        print(\"Warning: Missing values still exist in features after imputation!\")\n",
    "\n",
    "    # --- Feature Scaling ---\n",
    "    # Standardize numerical features. Fit scaler ONLY on training data.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_2025_scaled = scaler.transform(X_2025) # Transform 2025 data using scaler fitted on training data\n",
    "\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)\n",
    "    X_2025_scaled_df = pd.DataFrame(X_2025_scaled, columns=feature_cols, index=X_2025.index)\n",
    "\n",
    "    print(\"\\nFeatures scaled.\")\n",
    "    print(X_scaled_df.head())\n",
    "else:\n",
    "    print(\"Skipping data preparation due to missing master dataset.\")\n",
    "\n",
    "# ## 4. Model Training and Cross-Validation\n",
    "# Train and evaluate different regression models using K-Fold Cross-Validation to get robust performance metrics.\n",
    "\n",
    "if 'X_scaled_df' in locals() and X_scaled_df is not None:\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'XGBoost': XGBRegressor(random_state=42, n_estimators=100),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'Lasso': Lasso(random_state=42),\n",
    "        'Stacking': StackingRegressor(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestRegressor(random_state=42)),\n",
    "                ('xgb', XGBRegressor(random_state=42, n_estimators=50)) # Lower n_estimators for base models\n",
    "            ],\n",
    "            final_estimator=LinearRegression(),\n",
    "            cv=5 # Cross-validation for the stacking process\n",
    "        )\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    print(\"\\nStarting model training and cross-validation...\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Use negative mean squared error for cross_val_score\n",
    "        rmse_scores = np.sqrt(-cross_val_score(model, X_scaled_df, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1))\n",
    "        r2_scores = cross_val_score(model, X_scaled_df, y, cv=kf, scoring='r2', n_jobs=-1)\n",
    "        mae_scores = -cross_val_score(model, X_scaled_df, y, cv=kf, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        avg_r2 = np.mean(r2_scores)\n",
    "        avg_mae = np.mean(mae_scores)\n",
    "\n",
    "        print(f\"  {name} - Cross-Validation Results:\")\n",
    "        print(f\"    Avg RMSE: {avg_rmse:.3f} (Std: {np.std(rmse_scores):.3f})\")\n",
    "        print(f\"    Avg R2: {avg_r2:.3f} (Std: {np.std(r2_scores):.3f})\")\n",
    "        print(f\"    Avg MAE: {avg_mae:.3f} (Std: {np.std(mae_scores):.3f})\")\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Avg RMSE': avg_rmse,\n",
    "            'Std RMSE': np.std(rmse_scores),\n",
    "            'Avg R2': avg_r2,\n",
    "            'Std R2': np.std(r2_scores),\n",
    "            'Avg MAE': avg_mae,\n",
    "            'Std MAE': np.std(mae_scores)\n",
    "        })\n",
    "\n",
    "        # Fit the model on the full training data for later use (e.g., prediction)\n",
    "        model.fit(X_scaled_df, y)\n",
    "\n",
    "        if avg_rmse < best_rmse:\n",
    "            best_rmse = avg_rmse\n",
    "            best_model = {'Model': name, 'Trained Model': model}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='Avg RMSE')\n",
    "    print(\"--- Model Comparison ---\")\n",
    "    print(df_results)\n",
    "\n",
    "    print(f\"✅ Best Model (based on Avg RMSE): {best_model['Model']}\")\n",
    "\n",
    "    # Save the best model\n",
    "    model_save_path = os.path.join(model_dir, 'best_yield_prediction_model.pkl')\n",
    "    with open(model_save_path, 'wb') as f:\n",
    "        pickle.dump(best_model['Trained Model'], f)\n",
    "    print(f\"✅ Best trained model saved to {model_save_path}\")\n",
    "\n",
    "    # Save the scaler as well, needed for future predictions\n",
    "    scaler_save_path = os.path.join(model_dir, 'feature_scaler.pkl')\n",
    "    with open(scaler_save_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"✅ Feature scaler saved to {scaler_save_path}\")\n",
    "\n",
    "    # Save feature_cols for consistency when loading model later\n",
    "    feature_cols_path = os.path.join(model_dir, 'feature_columns.pkl')\n",
    "    with open(feature_cols_path, 'wb') as f:\n",
    "        pickle.dump(feature_cols, f)\n",
    "    print(f\"✅ Feature column names saved to {feature_cols_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping model training due to missing prepared data.\")\n",
    "\n",
    "# ## 5. 2025 Yield Prediction\n",
    "# Use the best trained model to predict coffee yields for the year 2025 (or any year with features but no observed yield).\n",
    "\n",
    "if 'best_model' in locals() and best_model is not None and 'X_2025_scaled_df' in locals() and X_2025_scaled_df is not None:\n",
    "    print(\"\\nMaking 2025 yield predictions...\")\n",
    "    predictions_2025 = best_model['Trained Model'].predict(X_2025_scaled_df)\n",
    "\n",
    "    # Add predictions back to the 2025 DataFrame\n",
    "    df_2025['predicted_yield_quintals_ha'] = predictions_2025\n",
    "\n",
    "    # Ensure predictions are non-negative\n",
    "    df_2025['predicted_yield_quintals_ha'] = df_2025['predicted_yield_quintals_ha'].apply(lambda x: max(0, x))\n",
    "\n",
    "    output_path = os.path.join(processed_data_dir, 'sidama_coffee_yield_predictions_2025.csv')\n",
    "    df_2025[['woreda_id', 'woreda_name', 'year', 'predicted_yield_quintals_ha']].to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ 2025 predictions saved to {output_path}: {df_2025.shape}\")\n",
    "    print(df_2025[['woreda_name', 'year', 'predicted_yield_quintals_ha']].head())\n",
    "else:\n",
    "    print(\"Skipping 2025 prediction due to missing model or 2025 data.\")\n",
    "\n",
    "# ## 6. Feature Importance Analysis\n",
    "# Understand which features contribute most to the model's predictions. This helps in interpreting the model and potentially identifying key drivers of coffee yield.\n",
    "\n",
    "if 'best_model' in locals() and best_model is not None and 'feature_cols' in locals():\n",
    "    print(\"\\n--- Feature Importance/Coefficients ---\")\n",
    "    if best_model['Model'] in ['Random Forest', 'XGBoost']:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': best_model['Trained Model'].feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(feature_importance.head(10))\n",
    "    elif best_model['Model'] in ['Ridge', 'Lasso']:\n",
    "        coefficients = best_model['Trained Model'].coef_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Coefficient': coefficients\n",
    "        }).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "        print(feature_importance.head(10))\n",
    "    elif best_model['Model'] == 'Stacking':\n",
    "        # For stacking, direct feature importance can be complex.\n",
    "        # Can inspect final_estimator_ coefficients if it's linear, or base models' importances.\n",
    "        # This is a simplified view.\n",
    "        print(\"Feature importance for Stacking Regressor is complex. Consider inspecting base models.\")\n",
    "        if hasattr(best_model['Trained Model'].final_estimator_, 'coef_'):\n",
    "             coefficients = best_model['Trained Model'].final_estimator_.coef_\n",
    "             meta_feature_names = [f'base_model_{i}' for i in range(len(best_model['Trained Model'].estimators))]\n",
    "             meta_importance = pd.DataFrame({\n",
    "                 'Meta-Feature (Base Model)': meta_feature_names,\n",
    "                 'Coefficient': coefficients\n",
    "             }).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "             print(\"Meta-Feature (Base Model) Coefficients:\")\n",
    "             print(meta_importance)\n",
    "    else:\n",
    "        print(\"Feature importance not directly available for this model type.\")\n",
    "else:\n",
    "    print(\"Skipping feature importance analysis due to missing model or feature columns.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
